{"meta":{"version":1,"warehouse":"4.0.2"},"models":{"Asset":[{"_id":"source/CNAME","path":"CNAME","modified":1,"renderable":0},{"_id":"themes/butterfly/source/css/index.styl","path":"css/index.styl","modified":1,"renderable":1},{"_id":"themes/butterfly/source/css/var.styl","path":"css/var.styl","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/404.jpg","path":"img/404.jpg","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/favicon.png","path":"img/favicon.png","modified":1,"renderable":1},{"_id":"themes/butterfly/source/img/friend_404.gif","path":"img/friend_404.gif","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/main.js","path":"js/main.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/tw_cn.js","path":"js/tw_cn.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/utils.js","path":"js/utils.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/search/algolia.js","path":"js/search/algolia.js","modified":1,"renderable":1},{"_id":"themes/butterfly/source/js/search/local-search.js","path":"js/search/local-search.js","modified":1,"renderable":1}],"Cache":[{"_id":"source/.DS_Store","hash":"0c0b7b7d26059170f1b979585a8023d4c4cdfb00","modified":1659872218418},{"_id":"source/CNAME","hash":"a6ba207a7f4d2e19b4a91bfb22a7caadc5bc6377","modified":1659870555000},{"_id":"source/_posts/Google-QA-Labeling-金牌方案分享.md","hash":"600475de6eee7704a838f8dbb242fae1c58a0fa4","modified":1659870555000},{"_id":"source/about/index.md","hash":"fc22969873e6360126a866337c229afea2d8f18a","modified":1659870555000},{"_id":"source/categories/index.md","hash":"57fa0f4bf9f270d078c1ae8bb7015e6c5b4e2bd7","modified":1659870555000},{"_id":"source/music/index.md","hash":"deee2f627dc43c9bef22f6150c1bf21262a1512f","modified":1659870555000},{"_id":"source/link/index.md","hash":"1f0b4608e2bd6174b18bc396e0b9c813f7cf259b","modified":1659870555000},{"_id":"source/_posts/CCF-BDCI房地产问答匹配.md","hash":"06c4165c5326ad7e258520d55c6e17c56e35612c","modified":1659870555000},{"_id":"source/movies/index.md","hash":"9b9bf87b00649ff9811fcc60d6cd6674c33ed979","modified":1659870555000},{"_id":"source/tags/index.md","hash":"b5d7ed50994ddfb4a5d496997b3f7644ce4d3174","modified":1659870555000},{"_id":"themes/butterfly/.DS_Store","hash":"c94c6161f38cf345a3dccf6bcc3dacee506b5ebf","modified":1659939654565},{"_id":"themes/butterfly/README_CN.md","hash":"01b4feffb432293223f540921ce2cde748b2d2fe","modified":1659939388738},{"_id":"themes/butterfly/package.json","hash":"3bcb70e12282156703626e238e3719611c34c3b7","modified":1659939388757},{"_id":"themes/butterfly/LICENSE","hash":"1128f8f91104ba9ef98d37eea6523a888dcfa5de","modified":1659939388737},{"_id":"themes/butterfly/README.md","hash":"cde88743f77bad1b463aa78049398b19f878090e","modified":1659939388737},{"_id":"themes/butterfly/plugins.yml","hash":"0194c4fb88a6be30d983a83ac22072bb4d799c9a","modified":1659939388757},{"_id":"themes/butterfly/.github/stale.yml","hash":"5e8ea535424e8112439135d21afc5262c0bc0b39","modified":1659939388737},{"_id":"themes/butterfly/_config.yml","hash":"a66c19ca42b443b76328019552bba9a3660e7540","modified":1659875513980},{"_id":"themes/butterfly/languages/default.yml","hash":"1e37a3695d50e3e61d7c36e58a6dac872a4a56cd","modified":1659939388738},{"_id":"themes/butterfly/languages/en.yml","hash":"d1bb560698eb8b0079495b7b18b44facb610f9fd","modified":1659939388738},{"_id":"themes/butterfly/languages/zh-TW.yml","hash":"947f794e862bb2813e36887f777bdb760f70a322","modified":1659939388738},{"_id":"themes/butterfly/layout/archive.pug","hash":"a0c034c2d319320a54046805e80b58dc48b7e233","modified":1659939388738},{"_id":"themes/butterfly/layout/category.pug","hash":"710708cfdb436bc875602abf096c919ccdf544db","modified":1659939388739},{"_id":"themes/butterfly/languages/zh-CN.yml","hash":"28b6f0c39155651d747eb595e0a283bc97be2e09","modified":1659939388738},{"_id":"themes/butterfly/layout/index.pug","hash":"e1c3146834c16e6077406180858add0a8183875a","modified":1659939388756},{"_id":"themes/butterfly/layout/post.pug","hash":"fc9f45252d78fcd15e4a82bfd144401cba5b169a","modified":1659939388756},{"_id":"themes/butterfly/layout/page.pug","hash":"baf469784aef227e4cc840550888554588e87a13","modified":1659939388756},{"_id":"themes/butterfly/layout/tag.pug","hash":"0440f42569df2676273c026a92384fa7729bc4e9","modified":1659939388757},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/config.yml","hash":"7dfe7189ffeaebb6db13842237f8e124649bea3d","modified":1659939388737},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/bug_report.yml","hash":"67e4f5a66d4b8cabadbaad0410628364ee75e0ae","modified":1659939388737},{"_id":"themes/butterfly/.github/ISSUE_TEMPLATE/feature_request.yml","hash":"996640605ed1e8e35182f0fd9a60a88783b24b03","modified":1659939388737},{"_id":"themes/butterfly/.github/workflows/publish.yml","hash":"05857c2f265246d8de00e31037f2720709540c09","modified":1659939388737},{"_id":"themes/butterfly/layout/includes/additional-js.pug","hash":"594a977ebe8d97e60fa3d7cb40fc260ded4d8a58","modified":1659939388739},{"_id":"themes/butterfly/layout/includes/404.pug","hash":"cb49f737aca272ccfeb62880bd651eccee72a129","modified":1659939388739},{"_id":"themes/butterfly/layout/includes/footer.pug","hash":"02390a5b6ae1f57497b22ba2e6be9f13cfb7acac","modified":1659939388739},{"_id":"themes/butterfly/layout/includes/head.pug","hash":"de8e618ec03295561c667a49e1c383213b856f6f","modified":1659939388739},{"_id":"themes/butterfly/layout/includes/layout.pug","hash":"a557280a25f63f4312afad63fc3303ec74165557","modified":1659939388741},{"_id":"themes/butterfly/layout/includes/pagination.pug","hash":"0b80f04950bd0fe5e6c4e7b7559adf4d0ce28436","modified":1659939388743},{"_id":"themes/butterfly/scripts/events/404.js","hash":"83cd7f73225ccad123afbd526ce1834eb1eb6a6d","modified":1659939388757},{"_id":"themes/butterfly/scripts/events/cdn.js","hash":"acfe765fb2c607bff7198630dbfa53a888c36357","modified":1659939388757},{"_id":"themes/butterfly/scripts/events/comment.js","hash":"a3d1f417965ca20253c36f9e93429f3df6268856","modified":1659939388758},{"_id":"themes/butterfly/scripts/events/init.js","hash":"b4940a5c73d3a5cd8bb5883e3041ecdd905a74e0","modified":1659939388758},{"_id":"themes/butterfly/scripts/events/stylus.js","hash":"9819f0996234fbd80d6c50a9e526c56ebf22588d","modified":1659939388758},{"_id":"themes/butterfly/scripts/events/welcome.js","hash":"3cfc46c749e2fd7ae9c2a17206238ed0e0e17e7d","modified":1659939388758},{"_id":"themes/butterfly/scripts/filters/post_lazyload.js","hash":"932df912976261929f809b7dbd4eb473e7787345","modified":1659939388758},{"_id":"themes/butterfly/scripts/filters/random_cover.js","hash":"21379ed2dccb69c43b893895c9d56238c11e5f43","modified":1659939388758},{"_id":"themes/butterfly/scripts/helpers/aside_archives.js","hash":"2ec66513d5322f185d2071acc052978ba9415a8e","modified":1659939388759},{"_id":"themes/butterfly/scripts/helpers/aside_categories.js","hash":"e00efdb5d02bc5c6eb4159e498af69fa61a7dbb9","modified":1659939388759},{"_id":"themes/butterfly/scripts/helpers/findArchiveLength.js","hash":"ee3e70098eb0849497d50b75e18cf4a27c397d52","modified":1659939388759},{"_id":"themes/butterfly/scripts/helpers/inject_head_js.js","hash":"b4cd617c619d1a0df93603721a6fa1317526174b","modified":1659939388759},{"_id":"themes/butterfly/scripts/helpers/page.js","hash":"763dab5c83f50c1c62fffc9a9dfedea29bb4e629","modified":1659939388759},{"_id":"themes/butterfly/scripts/tag/button.js","hash":"91d954f6e9fe6e571eb8ec9f8996294b2dc3688e","modified":1659939388760},{"_id":"themes/butterfly/scripts/helpers/related_post.js","hash":"d368a8830e506c8b5eb6512b709ec8db354d5ea1","modified":1659939388760},{"_id":"themes/butterfly/scripts/tag/gallery.js","hash":"f79c99f6c5b626c272dc2bed2b0250d6b91bb28a","modified":1659939388760},{"_id":"themes/butterfly/scripts/tag/flink.js","hash":"ab62919fa567b95fbe14889517abda649991b1ee","modified":1659939388760},{"_id":"themes/butterfly/scripts/tag/hide.js","hash":"396c3ab1bcf1c7693ad7e506eadd13016c6769b6","modified":1659939388760},{"_id":"themes/butterfly/scripts/tag/inlineImg.js","hash":"a43ee2c7871bdd93cb6beb804429e404570f7929","modified":1659939388761},{"_id":"themes/butterfly/scripts/tag/label.js","hash":"03b2afef41d02bd1045c89578a02402c28356006","modified":1659939388761},{"_id":"themes/butterfly/scripts/tag/mermaid.js","hash":"531808a290b8bdd66bac2faab211ada8e9646a37","modified":1659939388761},{"_id":"themes/butterfly/scripts/tag/note.js","hash":"d51812b43924f1bbf413c67499510dd125022005","modified":1659939388761},{"_id":"themes/butterfly/scripts/tag/tabs.js","hash":"6c6e415623d0fd39da016d9e353bb4f5cca444f5","modified":1659939388761},{"_id":"themes/butterfly/scripts/tag/timeline.js","hash":"300eb779588bf35a1b687d9f829d866074b707e3","modified":1659939388761},{"_id":"themes/butterfly/layout/includes/rightside.pug","hash":"699d0d2cff233628752956c4434125c8203f7d63","modified":1659939388744},{"_id":"themes/butterfly/layout/includes/sidebar.pug","hash":"8d39473ed112d113674a0f689f63fae06c72abd2","modified":1659939388744},{"_id":"themes/butterfly/source/css/index.styl","hash":"861998e4ac67a59529a8245a9130d68f826c9c12","modified":1659939388770},{"_id":"themes/butterfly/source/css/var.styl","hash":"4890a40366d6443f8b8942a4e9a6dce9fe3494f5","modified":1659939388770},{"_id":"themes/butterfly/source/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1659939388770},{"_id":"themes/butterfly/source/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1659939388770},{"_id":"themes/butterfly/source/js/main.js","hash":"73d2624ed465e4cfb1ebb00b2c8a24f5fc29bb21","modified":1659939388771},{"_id":"themes/butterfly/source/js/tw_cn.js","hash":"00053ce73210274b3679f42607edef1206eebc68","modified":1659939388772},{"_id":"themes/butterfly/source/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1659939388771},{"_id":"themes/butterfly/layout/includes/head/Open_Graph.pug","hash":"6c41f49a3e682067533dd9384e6e4511fc3a1349","modified":1659939388739},{"_id":"themes/butterfly/source/js/utils.js","hash":"0b95daada72abb5d64a1e3236049a60120e47cca","modified":1659939388773},{"_id":"themes/butterfly/layout/includes/head/analytics.pug","hash":"15530d9ac59c576d79af75dd687efe71e8d261b0","modified":1659939388739},{"_id":"themes/butterfly/layout/includes/head/config.pug","hash":"8f41fa9732ea654a10f6e666d9c782c7e27e5ea6","modified":1659939388739},{"_id":"themes/butterfly/layout/includes/head/google_adsense.pug","hash":"95a37e92b39c44bcbea4be7e29ddb3921c5b8220","modified":1659939388740},{"_id":"themes/butterfly/layout/includes/head/noscript.pug","hash":"d16ad2ee0ff5751fd7f8a5ce1b83935518674977","modified":1659939388740},{"_id":"themes/butterfly/layout/includes/head/config_site.pug","hash":"7df90c8e432e33716517ab918b0a125bc284041b","modified":1659939388740},{"_id":"themes/butterfly/layout/includes/head/preconnect.pug","hash":"65a23b5170204e55b813ce13a79d799b66b7382c","modified":1659939388740},{"_id":"themes/butterfly/layout/includes/head/site_verification.pug","hash":"e2e8d681f183f00ce5ee239c42d2e36b3744daad","modified":1659939388740},{"_id":"themes/butterfly/layout/includes/head/pwa.pug","hash":"3d492cfe645d37c94d30512e0b230b0a09913148","modified":1659939388740},{"_id":"themes/butterfly/layout/includes/header/index.pug","hash":"aa175e2254704335f4da09175e59ef2375ca7d03","modified":1659939388740},{"_id":"themes/butterfly/layout/includes/header/menu_item.pug","hash":"31346a210f4f9912c5b29f51d8f659913492f388","modified":1659939388741},{"_id":"themes/butterfly/layout/includes/header/nav.pug","hash":"78a3abd90bb3c18cd773d3d5abac3541e7f415e5","modified":1659939388741},{"_id":"themes/butterfly/layout/includes/page/categories.pug","hash":"5276a8d2835e05bd535fedc9f593a0ce8c3e8437","modified":1659939388742},{"_id":"themes/butterfly/layout/includes/header/social.pug","hash":"0d953e51d04a9294a64153c89c20f491a9ec42d4","modified":1659939388741},{"_id":"themes/butterfly/layout/includes/header/post-info.pug","hash":"8c2524e843adfce00b16e31ee355f14ce9ffb8ba","modified":1659939388741},{"_id":"themes/butterfly/layout/includes/page/default-page.pug","hash":"12c65c174d26a41821df9bad26cdf1087ec5b0ca","modified":1659939388743},{"_id":"themes/butterfly/layout/includes/loading/loading-js.pug","hash":"4cfcf0100e37ce91864703cd44f1cb99cb5493ea","modified":1659939388741},{"_id":"themes/butterfly/layout/includes/page/tags.pug","hash":"6311eda08e4515281c51bd49f43902a51832383c","modified":1659939388743},{"_id":"themes/butterfly/layout/includes/page/flink.pug","hash":"fed069baa9b383f57db32bb631115071d29bdc60","modified":1659939388743},{"_id":"themes/butterfly/layout/includes/loading/loading.pug","hash":"5276937fbcceb9d62879dc47be880cd469a27349","modified":1659939388742},{"_id":"themes/butterfly/layout/includes/mixins/article-sort.pug","hash":"2fb74d0b0e4b98749427c5a1a1b0acb6c85fadc4","modified":1659939388742},{"_id":"themes/butterfly/layout/includes/mixins/post-ui.pug","hash":"0d10532648d0335254f21a3b7d8676cb96ea8eba","modified":1659939388742},{"_id":"themes/butterfly/layout/includes/post/post-copyright.pug","hash":"ebecba46a5f4efe1c98a386df06c56e26fbd07b9","modified":1659939388743},{"_id":"themes/butterfly/layout/includes/widget/card_ad.pug","hash":"60dc48a7b5d89c2a49123c3fc5893ab9c57dd225","modified":1659939388754},{"_id":"themes/butterfly/layout/includes/post/reward.pug","hash":"864869c43fe5b5bb6f4ac6b13dd4bfb16ea47550","modified":1659939388743},{"_id":"themes/butterfly/layout/includes/widget/card_archives.pug","hash":"86897010fe71503e239887fd8f6a4f5851737be9","modified":1659939388754},{"_id":"themes/butterfly/layout/includes/widget/card_announcement.pug","hash":"ae392459ad401a083ca51ee0b27526b3c1e1faed","modified":1659939388754},{"_id":"themes/butterfly/layout/includes/widget/card_bottom_self.pug","hash":"13dc8ce922e2e2332fe6ad5856ebb5dbf9ea4444","modified":1659939388754},{"_id":"themes/butterfly/layout/includes/widget/card_categories.pug","hash":"d1a416d0a8a7916d0b1a41d73adc66f8c811e493","modified":1659939388755},{"_id":"themes/butterfly/layout/includes/widget/card_newest_comment.pug","hash":"6d93564a8bd13cb9b52ee5e178db3bcbf18b1bc6","modified":1659939388755},{"_id":"themes/butterfly/layout/includes/widget/card_author.pug","hash":"e37468e63db2a0ac09b65d21b7de3e62425bb455","modified":1659939388754},{"_id":"themes/butterfly/layout/includes/widget/card_post_toc.pug","hash":"3057a2f6f051355e35d3b205121af8735100eacf","modified":1659939388755},{"_id":"themes/butterfly/layout/includes/widget/card_recent_post.pug","hash":"9c1229af6ab48961021886882c473514101fba21","modified":1659939388755},{"_id":"themes/butterfly/layout/includes/widget/card_top_self.pug","hash":"ae67c6d4130a6c075058a9c1faea1648bcc6f83e","modified":1659939388755},{"_id":"themes/butterfly/layout/includes/widget/card_tags.pug","hash":"438aea3e713ed16b7559b9a80a9c5ec0221263df","modified":1659939388755},{"_id":"themes/butterfly/layout/includes/widget/index.pug","hash":"7fb096656c8a6c21a4b6a5100885b1081d6021ed","modified":1659939388756},{"_id":"themes/butterfly/layout/includes/widget/card_webinfo.pug","hash":"35ce167c5a275211bfc1fa3d49adfde5b404d98f","modified":1659939388756},{"_id":"themes/butterfly/layout/includes/third-party/aplayer.pug","hash":"c7cfade2b160380432c47eef4cd62273b6508c58","modified":1659939388744},{"_id":"themes/butterfly/layout/includes/third-party/effect.pug","hash":"6528e86656906117a1af6b90e0349c2c4651d5e1","modified":1659939388749},{"_id":"themes/butterfly/layout/includes/third-party/pangu.pug","hash":"0f024e36b8116118233e10118714bde304e01e12","modified":1659939388752},{"_id":"themes/butterfly/layout/includes/third-party/pjax.pug","hash":"6d6474ef186c18d9b4f334e1f735eadd6699effa","modified":1659939388752},{"_id":"themes/butterfly/layout/includes/third-party/prismjs.pug","hash":"ffb9ea15a2b54423cd4cd441e2d061b8233e9b58","modified":1659939388752},{"_id":"themes/butterfly/layout/includes/third-party/subtitle.pug","hash":"bae2f32ac96cebef600c1e37eaa8467c9a7e5d92","modified":1659939388753},{"_id":"themes/butterfly/source/css/_global/function.styl","hash":"644d520fe80cc82058467708ab82ccad313b0c27","modified":1659939388761},{"_id":"themes/butterfly/source/css/_global/index.styl","hash":"714f19e7d66df84938bd1b82b33d5667abe1f147","modified":1659939388762},{"_id":"themes/butterfly/source/css/_highlight/highlight.styl","hash":"2f95e99b8351fbecd9037a1bbdc3fee9d6ea8a77","modified":1659939388762},{"_id":"themes/butterfly/source/css/_highlight/theme.styl","hash":"bcd384c8b2aa0390c9eb69ac1abbfd1240ce1da4","modified":1659939388763},{"_id":"themes/butterfly/source/css/_layout/aside.styl","hash":"a0010d833ed30211601c1e0bbbc68e85b77428c6","modified":1659939388763},{"_id":"themes/butterfly/source/css/_layout/chat.styl","hash":"29f48f9370f245e6e575b5836bccf47eb5688d8b","modified":1659939388763},{"_id":"themes/butterfly/source/css/_layout/comments.styl","hash":"c61dccca690d486c3d9c29cf028d87b777385141","modified":1659939388763},{"_id":"themes/butterfly/source/css/_layout/footer.styl","hash":"26be2afa9d4e7016cf3c42a6cd166f01e8e4ad5c","modified":1659939388764},{"_id":"themes/butterfly/source/css/_layout/head.styl","hash":"d97c1722ce0fcc319f1f90ec2d51f9d746748e2b","modified":1659939388764},{"_id":"themes/butterfly/source/css/_layout/loading.styl","hash":"ef21990de28bd75dcd0f88b8d616e1a7a137502f","modified":1659939388764},{"_id":"themes/butterfly/source/css/_layout/pagination.styl","hash":"fb9f78bfbb79579f1d752cb73fb6d25c8418e0fd","modified":1659939388764},{"_id":"themes/butterfly/source/css/_layout/post.styl","hash":"15056fba0bd5a45ea8dc97eb557f6929ff16797a","modified":1659939388764},{"_id":"themes/butterfly/source/css/_layout/relatedposts.styl","hash":"d53de408cb27a2e704aba7f7402b7caebe0410d8","modified":1659939388764},{"_id":"themes/butterfly/source/css/_layout/reward.styl","hash":"c5cfed620708807a48076b5ee59b0ba84e29aa80","modified":1659939388764},{"_id":"themes/butterfly/source/css/_layout/rightside.styl","hash":"bd88ee30ebf8ca2e7b4d3a034c317fd61733921f","modified":1659939388765},{"_id":"themes/butterfly/source/css/_layout/sidebar.styl","hash":"631ca35a38bc4ac052e9caf47508ff1f99842fc7","modified":1659939388765},{"_id":"themes/butterfly/source/css/_layout/third-party.styl","hash":"8314e9749eb1ae40c4bae9735b7a6638b2d6876a","modified":1659939388765},{"_id":"themes/butterfly/source/css/_mode/darkmode.styl","hash":"a92984f566c97bb4179f34be79240af1552c6f17","modified":1659939388765},{"_id":"themes/butterfly/source/css/_page/404.styl","hash":"50dbb9e6d98c71ffe16741b8c1b0c1b9771efd2b","modified":1659939388766},{"_id":"themes/butterfly/source/css/_mode/readmode.styl","hash":"69f8e9414526dfda3af9a71c8e528fdd0ecbbfe5","modified":1659939388765},{"_id":"themes/butterfly/source/css/_page/archives.styl","hash":"6f4b4ede52305bce9b22c8c897dcbde8af6e2ce4","modified":1659939388766},{"_id":"themes/butterfly/source/css/_page/common.styl","hash":"a58d35d698885f1034dedbe99f7dbc1a801412c6","modified":1659939388766},{"_id":"themes/butterfly/source/css/_page/categories.styl","hash":"f01ee74948cedb44e53cd3bb1ef36b7d2778ede7","modified":1659939388766},{"_id":"themes/butterfly/source/css/_page/flink.styl","hash":"98d755b686ee833e9da10afaa40c4ec2bd66c19a","modified":1659939388766},{"_id":"themes/butterfly/source/css/_page/homepage.styl","hash":"8c90483d461e09cb06e91b16d8bb7b3205b0a40c","modified":1659939388767},{"_id":"themes/butterfly/source/css/_page/tags.styl","hash":"580feb7e8b0822a1be48ac380f8c5c53b1523321","modified":1659939388767},{"_id":"themes/butterfly/source/css/_search/algolia.styl","hash":"51e45625929d57c9df3ba9090af99b9b7bb9a15b","modified":1659939388767},{"_id":"themes/butterfly/source/css/_search/index.styl","hash":"39d61cbe0c1e937f83ba3b147afaa29b4de2f87d","modified":1659939388767},{"_id":"themes/butterfly/source/css/_search/local-search.styl","hash":"25e58a7a8bda4b73d0a0e551643ca01b09ccd7e5","modified":1659939388767},{"_id":"themes/butterfly/source/css/_tags/gallery.styl","hash":"a310e48f826a4cacc55d8e68f43806e5085554f6","modified":1659939388768},{"_id":"themes/butterfly/source/css/_tags/button.styl","hash":"45f0c32bdea117540f6b14ebac6450d7142bd710","modified":1659939388768},{"_id":"themes/butterfly/source/css/_tags/hexo.styl","hash":"d76c38adf1d9c1279ef4241835667789f5b736e0","modified":1659939388768},{"_id":"themes/butterfly/source/css/_tags/hide.styl","hash":"ce489ca2e249e2a3cf71584e20d84bdb022e3475","modified":1659939388768},{"_id":"themes/butterfly/source/css/_tags/inlineImg.styl","hash":"df9d405c33a9a68946b530410f64096bcb72560c","modified":1659939388769},{"_id":"themes/butterfly/source/css/_tags/label.styl","hash":"66c59e193d794cdb02cca7bd1dc4aea5a19d7e84","modified":1659939388769},{"_id":"themes/butterfly/source/css/_tags/tabs.styl","hash":"bf9568444dd54e39dc59b461323dcd38942f27d9","modified":1659939388769},{"_id":"themes/butterfly/source/css/_tags/note.styl","hash":"85ae91c83691ea4511f4277da1194a185251cc78","modified":1659939388769},{"_id":"themes/butterfly/source/css/_tags/timeline.styl","hash":"f071156d439556e7463ed4bc61ceee87170d5d08","modified":1659939388769},{"_id":"themes/butterfly/source/css/_third-party/normalize.min.css","hash":"2c18a1c9604af475b4749def8f1959df88d8b276","modified":1659939388769},{"_id":"themes/butterfly/source/js/search/algolia.js","hash":"9feb248552667c53ce1b19bc7a295215f8c77008","modified":1659939388772},{"_id":"themes/butterfly/source/js/search/local-search.js","hash":"3071a4208fdf89ad7e0031536dd6ffa7bc951e4d","modified":1659939388772},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/disqus.pug","hash":"d85c3737b5c9548553a78b757a7698df126a52cf","modified":1659939388744},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/index.pug","hash":"2be601477e1b81eee90a00c14ce5c8761d19a332","modified":1659939388745},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/remark42.pug","hash":"001e8be47854b891efe04013c240c38fed4185eb","modified":1659939388745},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/fb.pug","hash":"7848ec58c6ec03243abf80a3b22b4dc10f3edf53","modified":1659939388744},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/valine.pug","hash":"39427e107230a10790972349c9dd4c4f31d55eb7","modified":1659939388745},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/twikoo.pug","hash":"58406a7a3bf45815769f652bf3ef81e57dcd07eb","modified":1659939388745},{"_id":"themes/butterfly/layout/includes/third-party/card-post-count/waline.pug","hash":"5f648086a33a32d169a2f8d8c549c08aa02f67db","modified":1659939388745},{"_id":"themes/butterfly/layout/includes/third-party/chat/chatra.pug","hash":"481cd5053bafb1a19f623554a27d3aa077ea59c3","modified":1659939388746},{"_id":"themes/butterfly/layout/includes/third-party/chat/crisp.pug","hash":"76634112c64023177260d1317ae39cef2a68e35f","modified":1659939388746},{"_id":"themes/butterfly/layout/includes/third-party/chat/daovoice.pug","hash":"cfe63e7d26a6665df6aa32ca90868ad48e05ec04","modified":1659939388746},{"_id":"themes/butterfly/layout/includes/third-party/chat/gitter.pug","hash":"d1d2474420bf4edc2e43ccdff6f92b8b082143df","modified":1659939388746},{"_id":"themes/butterfly/layout/includes/third-party/chat/index.pug","hash":"3f05f8311ae559d768ee3d0925e84ed767c314d3","modified":1659939388746},{"_id":"themes/butterfly/layout/includes/third-party/chat/tidio.pug","hash":"24a926756c2300b9c561aaab6bd3a71fdd16e16d","modified":1659939388746},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqus.pug","hash":"8ec24c1939895ac0db2b2e8700bc9307b4ceb53c","modified":1659939388747},{"_id":"themes/butterfly/layout/includes/third-party/comments/disqusjs.pug","hash":"98ef20f8a3b10c1692f9b2b3c06033d2da8a8eae","modified":1659939388747},{"_id":"themes/butterfly/layout/includes/third-party/comments/facebook_comments.pug","hash":"2d8fc3fb8f9aec61400acf3c94070bd8539058f8","modified":1659939388747},{"_id":"themes/butterfly/layout/includes/third-party/comments/giscus.pug","hash":"591ef23c583690bd74af0cafb09af64ba5bd8151","modified":1659939388747},{"_id":"themes/butterfly/layout/includes/third-party/comments/gitalk.pug","hash":"22e2ef30fe5eb1db7566e89943c74ece029b2a8e","modified":1659939388748},{"_id":"themes/butterfly/layout/includes/third-party/comments/index.pug","hash":"2e26af16d359ba362fa611575d7f547848057c0c","modified":1659939388748},{"_id":"themes/butterfly/layout/includes/third-party/comments/js.pug","hash":"190b1cca42c7f73c50f62f07d0751449941bff3f","modified":1659939388748},{"_id":"themes/butterfly/layout/includes/third-party/comments/livere.pug","hash":"52ea8aa26b84d3ad38ae28cdf0f163e9ca8dced7","modified":1659939388748},{"_id":"themes/butterfly/layout/includes/third-party/comments/remark42.pug","hash":"e9bdf80d6796afc04eb809dbbe780d97f22c7fcd","modified":1659939388748},{"_id":"themes/butterfly/layout/includes/third-party/comments/twikoo.pug","hash":"e18fbd88d8942e53e771f29b26209ab735c5c567","modified":1659939388749},{"_id":"themes/butterfly/layout/includes/third-party/comments/utterances.pug","hash":"a737046e730eb7264606ba0536218964044492f9","modified":1659939388749},{"_id":"themes/butterfly/layout/includes/third-party/comments/valine.pug","hash":"e55b9c0f8ced231f47eb88bd7f4ec99f29c5c29d","modified":1659939388749},{"_id":"themes/butterfly/layout/includes/third-party/comments/waline.pug","hash":"15462d1ed04651ad3b430c682842ac400f6f9b47","modified":1659939388749},{"_id":"themes/butterfly/layout/includes/third-party/math/index.pug","hash":"b8ae5fd7d74e1edcef21f5004fc96147e064d219","modified":1659939388750},{"_id":"themes/butterfly/layout/includes/third-party/math/katex.pug","hash":"dfcbd9881be569ea420eff1a6b00e4f4dbe2138e","modified":1659939388750},{"_id":"themes/butterfly/layout/includes/third-party/math/mathjax.pug","hash":"f4dc7d02c8192979404ae9e134c5048d3d0a76e2","modified":1659939388750},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/disqus-comment.pug","hash":"04b2a5882e789a988e41d45abe606f0617b08e38","modified":1659939388750},{"_id":"themes/butterfly/layout/includes/third-party/math/mermaid.pug","hash":"8e33aca36a4d3ae9e041ba05ced8eff56ae38f77","modified":1659939388750},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/github-issues.pug","hash":"e846ddfe4a63b15d1416f6055f5756af5e3da7c6","modified":1659939388751},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/index.pug","hash":"8ca02f97bfa93fff9cce5b8eb8feb234e7beeb98","modified":1659939388751},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/remark42.pug","hash":"ab167c00da4506f591b96f0591bf5bd214a26d4b","modified":1659939388751},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/twikoo-comment.pug","hash":"233907dd7f5b5f33412701d2ccffbc0bbae8707b","modified":1659939388751},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/valine.pug","hash":"d19e1c2c0a50f0e4547d71a17b9be88e8152f17c","modified":1659939388751},{"_id":"themes/butterfly/layout/includes/third-party/newest-comments/waline.pug","hash":"dd0bc119029b62dce5dc965d5de7377e438fa29a","modified":1659939388751},{"_id":"themes/butterfly/layout/includes/third-party/search/algolia.pug","hash":"e8245d0b4933129bb1c485d8de11a9e52e676348","modified":1659939388752},{"_id":"themes/butterfly/layout/includes/third-party/search/index.pug","hash":"da3b9437d061ee68dbc383057db5c73034c49605","modified":1659939388752},{"_id":"themes/butterfly/layout/includes/third-party/search/local-search.pug","hash":"178c9cdcc4ce5a006885b24ce4a3d624e4734899","modified":1659939388753},{"_id":"themes/butterfly/layout/includes/third-party/share/add-this.pug","hash":"2980f1889226ca981aa23b8eb1853fde26dcf89a","modified":1659939388753},{"_id":"themes/butterfly/layout/includes/third-party/share/addtoany.pug","hash":"85c92f8a7e44d7cd1c86f089a05be438535e5362","modified":1659939388753},{"_id":"themes/butterfly/layout/includes/third-party/share/index.pug","hash":"4c4a9c15215ae8ac5eadb0e086b278f76db9ee92","modified":1659939388753},{"_id":"themes/butterfly/layout/includes/third-party/share/share-js.pug","hash":"f61d63724ea5c5f352568b3a16bde023affefbe5","modified":1659939388753},{"_id":"themes/butterfly/source/css/_highlight/highlight/diff.styl","hash":"cf1fae641c927621a4df1be5ca4a853b9b526e23","modified":1659939388762},{"_id":"themes/butterfly/source/css/_highlight/highlight/index.styl","hash":"18804c58239d95798fa86d0597f32d7f7dd30051","modified":1659939388762},{"_id":"themes/butterfly/source/css/_highlight/prismjs/diff.styl","hash":"5972c61f5125068cbe0af279a0c93a54847fdc3b","modified":1659939388762},{"_id":"themes/butterfly/source/css/_highlight/prismjs/index.styl","hash":"5dc2e0bcae9a54bfb9bdcc82d02ae5a3cf1ca97d","modified":1659939388762},{"_id":"themes/butterfly/source/css/_highlight/prismjs/line-number.styl","hash":"8970cc1916c982b64a1478792b2822d1d31e276d","modified":1659939388762},{"_id":"public/movies/index.html","hash":"ffc89369583cd77c3b508fc8b157846a0d813667","modified":1659939696388},{"_id":"public/music/index.html","hash":"d260033978de0a88ca931d8475f360cab4d6418f","modified":1659939696388},{"_id":"public/categories/index.html","hash":"f752953550c6ea2f77e431607240b34c25de3a26","modified":1659939696388},{"_id":"public/about/index.html","hash":"9fbebae3d10b9234a0a5dbc56c1291faed2e9dda","modified":1659939696388},{"_id":"public/link/index.html","hash":"b876128fa14a0258255b3de4ccd94168f0b38cd2","modified":1659939696388},{"_id":"public/tags/index.html","hash":"04acd170052864861e26d6407c955d5ec04294a2","modified":1659939696388},{"_id":"public/2021/01/29/CCF-BDCI房地产问答匹配/index.html","hash":"78085d270e0ef06d4196f6194b7a8db5e753f83a","modified":1659939696388},{"_id":"public/2020/03/12/Google-QA-Labeling-金牌方案分享/index.html","hash":"234bd09d1a9760c50a7ee1e1a435e9fcbb0c89ce","modified":1659939696388},{"_id":"public/archives/index.html","hash":"8252814d40e6643b6a10e186563ff420478ff0c9","modified":1659939696388},{"_id":"public/archives/2020/index.html","hash":"9d885277b5140b4b1ab6e665deb3aab2b3a1d3d1","modified":1659939696388},{"_id":"public/archives/2020/03/index.html","hash":"ee4829732b538e60820be64234dfa9980714b9dc","modified":1659939696388},{"_id":"public/archives/2021/index.html","hash":"e788c9c3aaf25b6df891c8cd866193552f497346","modified":1659939696388},{"_id":"public/archives/2021/01/index.html","hash":"a623c0712a97ce5fc9277e722e89e9042fcd7871","modified":1659939696388},{"_id":"public/categories/competitions/index.html","hash":"428c9ebba913b0a20afd52a8d318466a4a614b19","modified":1659939696388},{"_id":"public/index.html","hash":"39f881a02d5eea44421c7be57265442863a76e8e","modified":1659939696388},{"_id":"public/tags/nlp/index.html","hash":"334caee0b34090c41cb92019739cf2c3082a8fbb","modified":1659939696388},{"_id":"public/tags/text-classifier/index.html","hash":"0eb23ebed32143388f88bc4c9b7e871c849aa406","modified":1659939696388},{"_id":"public/tags/qa-match/index.html","hash":"c48046545fb6d92998e5fedb528779456d31a699","modified":1659939696388},{"_id":"public/tags/kaggle/index.html","hash":"0bde1cd5d8fab1e0358f806497ba1e30261a6212","modified":1659939696388},{"_id":"public/CNAME","hash":"a6ba207a7f4d2e19b4a91bfb22a7caadc5bc6377","modified":1659939696388},{"_id":"public/img/favicon.png","hash":"3cf89864b4f6c9b532522a4d260a2e887971c92d","modified":1659939696388},{"_id":"public/img/404.jpg","hash":"fb4489bc1d30c93d28f7332158c1c6c1416148de","modified":1659939696388},{"_id":"public/img/friend_404.gif","hash":"8d2d0ebef70a8eb07329f57e645889b0e420fa48","modified":1659939696388},{"_id":"public/css/var.css","hash":"da39a3ee5e6b4b0d3255bfef95601890afd80709","modified":1659939696388},{"_id":"public/js/utils.js","hash":"0b95daada72abb5d64a1e3236049a60120e47cca","modified":1659939696388},{"_id":"public/js/search/local-search.js","hash":"3071a4208fdf89ad7e0031536dd6ffa7bc951e4d","modified":1659939696388},{"_id":"public/js/search/algolia.js","hash":"9feb248552667c53ce1b19bc7a295215f8c77008","modified":1659939696388},{"_id":"public/css/index.css","hash":"89c7f1eb8e412bf09faa8ac096fbb4d247b994ff","modified":1659939696388},{"_id":"public/js/main.js","hash":"73d2624ed465e4cfb1ebb00b2c8a24f5fc29bb21","modified":1659939696388},{"_id":"public/js/tw_cn.js","hash":"00053ce73210274b3679f42607edef1206eebc68","modified":1659939696388}],"Category":[{"name":"competitions","_id":"cl6kd8j8n00040vs6873obs6r"}],"Data":[],"Page":[{"title":"movies","date":"2022-08-07T10:59:43.000Z","type":"movies","_content":"","source":"movies/index.md","raw":"---\ntitle: movies\ndate: 2022-08-07 18:59:43\ntype: \"movies\"\n---\n","updated":"2022-08-07T10:59:43.000Z","path":"movies/index.html","comments":1,"layout":"page","_id":"cl6kd8j8f00000vs6ggxihz3g","content":"","site":{"data":{}},"cover":"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7","excerpt":"","more":""},{"title":"music","date":"2022-08-07T10:59:38.000Z","type":"music","_content":"","source":"music/index.md","raw":"---\ntitle: music\ndate: 2022-08-07 18:59:38\ntype: \"music\"\n---\n","updated":"2022-08-07T10:59:38.000Z","path":"music/index.html","comments":1,"layout":"page","_id":"cl6kd8j8m00020vs62aa55gl8","content":"","site":{"data":{}},"cover":"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7","excerpt":"","more":""},{"title":"categories","date":"2022-08-07T10:59:11.000Z","type":"categories","_content":"","source":"categories/index.md","raw":"---\ntitle: categories\ndate: 2022-08-07 18:59:11\ntype: \"categories\"\n---\n","updated":"2022-08-07T10:59:11.000Z","path":"categories/index.html","comments":1,"layout":"page","_id":"cl6kd8j8p00060vs6azxfadx4","content":"","site":{"data":{}},"cover":"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7","excerpt":"","more":""},{"title":"about","date":"2022-08-07T10:59:27.000Z","type":"about","_content":"","source":"about/index.md","raw":"---\ntitle: about\ndate: 2022-08-07 18:59:27\ntype: \"about\"\n---\n","updated":"2022-08-07T10:59:27.000Z","path":"about/index.html","comments":1,"layout":"page","_id":"cl6kd8j8q00070vs63h80ad1m","content":"","site":{"data":{}},"cover":"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7","excerpt":"","more":""},{"title":"link","date":"2022-08-07T10:59:22.000Z","type":"link","_content":"","source":"link/index.md","raw":"---\ntitle: link\ndate: 2022-08-07 18:59:22\ntype: \"link\"\n---\n","updated":"2022-08-07T10:59:22.000Z","path":"link/index.html","comments":1,"layout":"page","_id":"cl6kd8j8q00080vs6dhun09gs","content":"","site":{"data":{}},"cover":"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7","excerpt":"","more":""},{"title":"tags","date":"2022-08-07T10:58:51.000Z","type":"tags","_content":"","source":"tags/index.md","raw":"---\ntitle: tags\ndate: 2022-08-07 18:58:51\ntype: \"tags\"\n---\n","updated":"2022-08-07T10:58:51.000Z","path":"tags/index.html","comments":1,"layout":"page","_id":"cl6kd8j8s000b0vs69h7ud3u6","content":"","site":{"data":{}},"cover":"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7","excerpt":"","more":""}],"Post":[{"layout":"blog","title":"CCF BDCI房地产问答匹配第二名解决方案","date":"2021-01-29T13:11:18.000Z","_content":"\n\n## 摘要\n房产行业聊天匹配问题本质上面是句子对的分类问题，即判断问题和回答是否匹配的文本分类问题。但是问答匹配不同的是除了依赖当前的回答，往往是在一个多伦对话场景下的产生背景，在本题目中通过对数据的分析，发现通过id问题下面的相邻id的问答具有上下文的关系，我们在设计模型的输入的时候引入了上下文的信息，使得模型能够学习到更多的因果关系。同时近年来基于transformer结构的预训练模型横扫各大nlp任务的SOTA，所以在模型结构方面我们也是采用了基于transformer的预训练语言模型结合任务做出了创新性的设计，取得了比较好的效果。我们的模型最终取得了线上第二的成绩。\n\n\n## 关键词\n文本分类、上下文信息、预训练模型、Transformer \n\n<!--more-->\n\n\n\n## 1.任务简介\n给定IM交流片段，片段包含一个客户问题以及随后的经纪人若干IM消息，从这些随后的经纪人消息中找出一个是对客户问题的回答。简单来说即是判断问题和回答这个句子对是不是匹配的。\n\n\n## 2.预训练语言模型简介\n\n### 2.1 BERT预训练模型\n2018年google公司AI团队新发布的BERT模型[1]，在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩：全部两个衡量指标上全面超越人类，并且还在11种不同NLP测试中创出最佳成绩，包括将GLUE基准推至80.4％（绝对改进7.6％），MultiNLI准确度达到86.7% （绝对改进率5.6％）等。BERT为NLP带来了里程碑式的改变，也是NLP领域近年来最重要的进展。    Bert base版本使用了12层的transformer [2] encoder层部分作为编码器来对文本输入提取语义特征，用空间向量来表示。Transformer通过self-attention机制使得相同的字在不同的语境下面有不同的空间向量表示。同时相比于传统的rnn,cnn特征提取层来说，突破了rnn不能并行计算的限制，相比cnn计算两个位置之间的关联所需的操作次数不随长度的增长。\n      ![transformer](/image/transformer.png)\n       图1：transformer网络结构Bert使用了两个预训练任务，来训练模型的权重。一个是mask language model， 即通过对输入的文本进行部分的随机替换成[MASK]字符，然后让模型来预测被[ MASK]的字符是什么，类似于完形填空任务。另外一个就是句子对分类任务，即判断相邻的句子是不是上下文关系。  \n\n       ![bert_finetune](/image/bert_finetune.png)\n\n       图2：bert预训练和finetune\n       \n       \n       \n### 2.2 BERT-WWM预训练模型\nbert-wwm [3]是谷歌在2019年5月31日发布的一项BERT的升级版本，主要更改了原预训练阶段的训练样本生成策略。 简单来说，原有基于WordPiece的分词方式会把一个完整的词切分成若干个子词，在生成训练样本时，这些被分开的子词会随机被mask。 在全词Mask中，如果一个完整的词的部分WordPiece子词被mask，则同属该词的其他部分也会被mask，即全词Mask。\n\n\n\n### 2.3 MACBERT预训练模型\nmacbert [4]作者针对Bert在做MLM预训练的时候使用的 [MASK]替换输入，但是在做别的下游任务finetune的时候是没有[MASK]输入的，这就导致了预训练任务输入和finetune输入的差异问题。不使用[MASK]token进行mask，因为在token微调阶段从未出现过[MASK]，而是通过使用同义词替换的方法进行替换。\n\n\n\n\n\n## 3.解决方案\n\n### 3.1 数据处理\n通过数据分析，我们发现对同个相同的问题id下面，相邻的两个问题id部分存在上下文关系。 所以为了让模型能够学习到更多的上下文关系我们在模型的输入加入了相邻的文本的信息：\n[CLS]问题[SEP]问答[START]相邻的上个回答[INSERT]相邻的下个回答[SEP]\n\n\n\n### 3.2 模型结构\n\n们设计了pytorch和tensorflow两个版本的模型。并且两个模型的单模型效果都在初赛上面进入了前5的成绩，由于两个模型不仅在框架上面，网络结构和输入上面也存在一定的差异，使得组队后，进行模型融合的时候，带来了比较大的提升。 两个模型融合直接进入了第二名的成绩和第一名差距也在一个千分点内。模型1:pytorch 版本，模型输入采用了下面的输入结构：[CLS]问题[SEP]问答[START]相邻的上个回答[INSERT]相邻的下个回答[SEP]。网络上面采用了bert等预训练模型作为特征提取层，由于bert不同层transformer提取出来的语义存在较大的差异，在不同语法上面侧重点不一样，我们设计了动态融合层，使用不同transformer层动态的加权方法来作为最后的表征。  \n![pytorch model](/image/beike3.png)\n图3：pytorch版本网络模型结构模型2: tensorflow版本。tf版模型将问题和回答按顺序拼接并使用“[SEP]”字符分割，并在问题前后插入“[unused1]”，回答前后按顺序插入“[unused2]”,“[unused3]”等字符后作为输入。模型可以大致分为编码层， 特征抽取层以及输出层。考虑到bert的不同层输出具有不同的语义信息，以及transformer对临近字符信息抽取能力较弱，编码层选择对bert各层输出加权融合，并输入bilstm强化对临近字符的抽取能力，最后选择bert各层的加权值以及bilstm的输出值的拼接向量作为编码层的输出。借鉴bert在信息抽取领域一些有效的结构，模型的特征抽取层被设计为：1.将[SEP],[unused1],[unused2]等特殊字符在编码层后的输出做为表征1；2.将回答，问题经过一层卷积和max-pooling的输出拼接值作为表征2；3. 将表征1，表征2拼接作为最终问题和回答的表征；4.最后将问题和回答的表征通过类似gate结构进行融合作为特征抽取层的输出。输出层简单的将上一层的输出输入dense加softmax得到是回答与不是回答的概率。\n![tensorflow model](/image/beike4.png)\n图4：tensorflow版本网络模型结构\n\n\n\n### 3.3提分Trick\n\n1.模型输入引入上下文信息提升巨大。  \n2.使用对抗学习带来5个千分点的提升。  \n3.在比赛数据语料上面继续做房产领域的的预训练带来5个千分点左右的提升。  \n4.使用动态加权层来融合预训练模型不同层的输出带来3个左右的千分点的提升。\n\n\n\n### 3.4 模型融合\n我们pytorch和tensorflow两个版本的单模型都有着很好的效果在A榜能进入前5。后面由于竞争比较激烈，我们采用了多模型集成学习的方法，基于pytorch和tensorflow两个版本分别跑了bert-wwm和mac-bert两个预训练的交叉验证的结果作为特征，为了防止模型过拟合，采用了线形模型作为基模型。最终取得了A榜第二和B榜第二的成绩。和第一名仅仅差距在一个千分点内。\n\n\n\n## 4.参考\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Association for Computational Linguistics (NAACL).\n\n[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000–6010. \n[3] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. 2019a. Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101.\n。\n[4] Cui, Yiming  and Che, Wanxiang  and Liu, Ting  and Qin, Bing Wang, Shijin and Hu, Guoping . 2020. Revisiting Pre-Trained Models for chinese Natural Language Processing. EMNLP\t\n\n\n","source":"_posts/CCF-BDCI房地产问答匹配.md","raw":"---\nlayout: blog\ntitle: CCF BDCI房地产问答匹配第二名解决方案\ndate: 2021-01-29 21:11:18\ntags:\n  - nlp\n  - text classifier\n  - qa match\n\ncategories:\n  - competitions\n\n\n---\n\n\n## 摘要\n房产行业聊天匹配问题本质上面是句子对的分类问题，即判断问题和回答是否匹配的文本分类问题。但是问答匹配不同的是除了依赖当前的回答，往往是在一个多伦对话场景下的产生背景，在本题目中通过对数据的分析，发现通过id问题下面的相邻id的问答具有上下文的关系，我们在设计模型的输入的时候引入了上下文的信息，使得模型能够学习到更多的因果关系。同时近年来基于transformer结构的预训练模型横扫各大nlp任务的SOTA，所以在模型结构方面我们也是采用了基于transformer的预训练语言模型结合任务做出了创新性的设计，取得了比较好的效果。我们的模型最终取得了线上第二的成绩。\n\n\n## 关键词\n文本分类、上下文信息、预训练模型、Transformer \n\n<!--more-->\n\n\n\n## 1.任务简介\n给定IM交流片段，片段包含一个客户问题以及随后的经纪人若干IM消息，从这些随后的经纪人消息中找出一个是对客户问题的回答。简单来说即是判断问题和回答这个句子对是不是匹配的。\n\n\n## 2.预训练语言模型简介\n\n### 2.1 BERT预训练模型\n2018年google公司AI团队新发布的BERT模型[1]，在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩：全部两个衡量指标上全面超越人类，并且还在11种不同NLP测试中创出最佳成绩，包括将GLUE基准推至80.4％（绝对改进7.6％），MultiNLI准确度达到86.7% （绝对改进率5.6％）等。BERT为NLP带来了里程碑式的改变，也是NLP领域近年来最重要的进展。    Bert base版本使用了12层的transformer [2] encoder层部分作为编码器来对文本输入提取语义特征，用空间向量来表示。Transformer通过self-attention机制使得相同的字在不同的语境下面有不同的空间向量表示。同时相比于传统的rnn,cnn特征提取层来说，突破了rnn不能并行计算的限制，相比cnn计算两个位置之间的关联所需的操作次数不随长度的增长。\n      ![transformer](/image/transformer.png)\n       图1：transformer网络结构Bert使用了两个预训练任务，来训练模型的权重。一个是mask language model， 即通过对输入的文本进行部分的随机替换成[MASK]字符，然后让模型来预测被[ MASK]的字符是什么，类似于完形填空任务。另外一个就是句子对分类任务，即判断相邻的句子是不是上下文关系。  \n\n       ![bert_finetune](/image/bert_finetune.png)\n\n       图2：bert预训练和finetune\n       \n       \n       \n### 2.2 BERT-WWM预训练模型\nbert-wwm [3]是谷歌在2019年5月31日发布的一项BERT的升级版本，主要更改了原预训练阶段的训练样本生成策略。 简单来说，原有基于WordPiece的分词方式会把一个完整的词切分成若干个子词，在生成训练样本时，这些被分开的子词会随机被mask。 在全词Mask中，如果一个完整的词的部分WordPiece子词被mask，则同属该词的其他部分也会被mask，即全词Mask。\n\n\n\n### 2.3 MACBERT预训练模型\nmacbert [4]作者针对Bert在做MLM预训练的时候使用的 [MASK]替换输入，但是在做别的下游任务finetune的时候是没有[MASK]输入的，这就导致了预训练任务输入和finetune输入的差异问题。不使用[MASK]token进行mask，因为在token微调阶段从未出现过[MASK]，而是通过使用同义词替换的方法进行替换。\n\n\n\n\n\n## 3.解决方案\n\n### 3.1 数据处理\n通过数据分析，我们发现对同个相同的问题id下面，相邻的两个问题id部分存在上下文关系。 所以为了让模型能够学习到更多的上下文关系我们在模型的输入加入了相邻的文本的信息：\n[CLS]问题[SEP]问答[START]相邻的上个回答[INSERT]相邻的下个回答[SEP]\n\n\n\n### 3.2 模型结构\n\n们设计了pytorch和tensorflow两个版本的模型。并且两个模型的单模型效果都在初赛上面进入了前5的成绩，由于两个模型不仅在框架上面，网络结构和输入上面也存在一定的差异，使得组队后，进行模型融合的时候，带来了比较大的提升。 两个模型融合直接进入了第二名的成绩和第一名差距也在一个千分点内。模型1:pytorch 版本，模型输入采用了下面的输入结构：[CLS]问题[SEP]问答[START]相邻的上个回答[INSERT]相邻的下个回答[SEP]。网络上面采用了bert等预训练模型作为特征提取层，由于bert不同层transformer提取出来的语义存在较大的差异，在不同语法上面侧重点不一样，我们设计了动态融合层，使用不同transformer层动态的加权方法来作为最后的表征。  \n![pytorch model](/image/beike3.png)\n图3：pytorch版本网络模型结构模型2: tensorflow版本。tf版模型将问题和回答按顺序拼接并使用“[SEP]”字符分割，并在问题前后插入“[unused1]”，回答前后按顺序插入“[unused2]”,“[unused3]”等字符后作为输入。模型可以大致分为编码层， 特征抽取层以及输出层。考虑到bert的不同层输出具有不同的语义信息，以及transformer对临近字符信息抽取能力较弱，编码层选择对bert各层输出加权融合，并输入bilstm强化对临近字符的抽取能力，最后选择bert各层的加权值以及bilstm的输出值的拼接向量作为编码层的输出。借鉴bert在信息抽取领域一些有效的结构，模型的特征抽取层被设计为：1.将[SEP],[unused1],[unused2]等特殊字符在编码层后的输出做为表征1；2.将回答，问题经过一层卷积和max-pooling的输出拼接值作为表征2；3. 将表征1，表征2拼接作为最终问题和回答的表征；4.最后将问题和回答的表征通过类似gate结构进行融合作为特征抽取层的输出。输出层简单的将上一层的输出输入dense加softmax得到是回答与不是回答的概率。\n![tensorflow model](/image/beike4.png)\n图4：tensorflow版本网络模型结构\n\n\n\n### 3.3提分Trick\n\n1.模型输入引入上下文信息提升巨大。  \n2.使用对抗学习带来5个千分点的提升。  \n3.在比赛数据语料上面继续做房产领域的的预训练带来5个千分点左右的提升。  \n4.使用动态加权层来融合预训练模型不同层的输出带来3个左右的千分点的提升。\n\n\n\n### 3.4 模型融合\n我们pytorch和tensorflow两个版本的单模型都有着很好的效果在A榜能进入前5。后面由于竞争比较激烈，我们采用了多模型集成学习的方法，基于pytorch和tensorflow两个版本分别跑了bert-wwm和mac-bert两个预训练的交叉验证的结果作为特征，为了防止模型过拟合，采用了线形模型作为基模型。最终取得了A榜第二和B榜第二的成绩。和第一名仅仅差距在一个千分点内。\n\n\n\n## 4.参考\n[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Association for Computational Linguistics (NAACL).\n\n[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000–6010. \n[3] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. 2019a. Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101.\n。\n[4] Cui, Yiming  and Che, Wanxiang  and Liu, Ting  and Qin, Bing Wang, Shijin and Hu, Guoping . 2020. Revisiting Pre-Trained Models for chinese Natural Language Processing. EMNLP\t\n\n\n","slug":"CCF-BDCI房地产问答匹配","published":1,"updated":"2021-01-29T13:11:18.000Z","comments":1,"photos":[],"link":"","_id":"cl6kd8j8j00010vs69odh859q","content":"<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>房产行业聊天匹配问题本质上面是句子对的分类问题，即判断问题和回答是否匹配的文本分类问题。但是问答匹配不同的是除了依赖当前的回答，往往是在一个多伦对话场景下的产生背景，在本题目中通过对数据的分析，发现通过id问题下面的相邻id的问答具有上下文的关系，我们在设计模型的输入的时候引入了上下文的信息，使得模型能够学习到更多的因果关系。同时近年来基于transformer结构的预训练模型横扫各大nlp任务的SOTA，所以在模型结构方面我们也是采用了基于transformer的预训练语言模型结合任务做出了创新性的设计，取得了比较好的效果。我们的模型最终取得了线上第二的成绩。</p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>文本分类、上下文信息、预训练模型、Transformer </p>\n<span id=\"more\"></span>\n\n\n\n<h2 id=\"1-任务简介\"><a href=\"#1-任务简介\" class=\"headerlink\" title=\"1.任务简介\"></a>1.任务简介</h2><p>给定IM交流片段，片段包含一个客户问题以及随后的经纪人若干IM消息，从这些随后的经纪人消息中找出一个是对客户问题的回答。简单来说即是判断问题和回答这个句子对是不是匹配的。</p>\n<h2 id=\"2-预训练语言模型简介\"><a href=\"#2-预训练语言模型简介\" class=\"headerlink\" title=\"2.预训练语言模型简介\"></a>2.预训练语言模型简介</h2><h3 id=\"2-1-BERT预训练模型\"><a href=\"#2-1-BERT预训练模型\" class=\"headerlink\" title=\"2.1 BERT预训练模型\"></a>2.1 BERT预训练模型</h3><p>2018年google公司AI团队新发布的BERT模型[1]，在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩：全部两个衡量指标上全面超越人类，并且还在11种不同NLP测试中创出最佳成绩，包括将GLUE基准推至80.4％（绝对改进7.6％），MultiNLI准确度达到86.7% （绝对改进率5.6％）等。BERT为NLP带来了里程碑式的改变，也是NLP领域近年来最重要的进展。    Bert base版本使用了12层的transformer [2] encoder层部分作为编码器来对文本输入提取语义特征，用空间向量来表示。Transformer通过self-attention机制使得相同的字在不同的语境下面有不同的空间向量表示。同时相比于传统的rnn,cnn特征提取层来说，突破了rnn不能并行计算的限制，相比cnn计算两个位置之间的关联所需的操作次数不随长度的增长。<br>      <img src=\"/image/transformer.png\" alt=\"transformer\"><br>       图1：transformer网络结构Bert使用了两个预训练任务，来训练模型的权重。一个是mask language model， 即通过对输入的文本进行部分的随机替换成[MASK]字符，然后让模型来预测被[ MASK]的字符是什么，类似于完形填空任务。另外一个就是句子对分类任务，即判断相邻的句子是不是上下文关系。  </p>\n<pre><code>   ![bert_finetune](/image/bert_finetune.png)\n\n   图2：bert预训练和finetune\n   \n   \n   \n</code></pre>\n<h3 id=\"2-2-BERT-WWM预训练模型\"><a href=\"#2-2-BERT-WWM预训练模型\" class=\"headerlink\" title=\"2.2 BERT-WWM预训练模型\"></a>2.2 BERT-WWM预训练模型</h3><p>bert-wwm [3]是谷歌在2019年5月31日发布的一项BERT的升级版本，主要更改了原预训练阶段的训练样本生成策略。 简单来说，原有基于WordPiece的分词方式会把一个完整的词切分成若干个子词，在生成训练样本时，这些被分开的子词会随机被mask。 在全词Mask中，如果一个完整的词的部分WordPiece子词被mask，则同属该词的其他部分也会被mask，即全词Mask。</p>\n<h3 id=\"2-3-MACBERT预训练模型\"><a href=\"#2-3-MACBERT预训练模型\" class=\"headerlink\" title=\"2.3 MACBERT预训练模型\"></a>2.3 MACBERT预训练模型</h3><p>macbert [4]作者针对Bert在做MLM预训练的时候使用的 [MASK]替换输入，但是在做别的下游任务finetune的时候是没有[MASK]输入的，这就导致了预训练任务输入和finetune输入的差异问题。不使用[MASK]token进行mask，因为在token微调阶段从未出现过[MASK]，而是通过使用同义词替换的方法进行替换。</p>\n<h2 id=\"3-解决方案\"><a href=\"#3-解决方案\" class=\"headerlink\" title=\"3.解决方案\"></a>3.解决方案</h2><h3 id=\"3-1-数据处理\"><a href=\"#3-1-数据处理\" class=\"headerlink\" title=\"3.1 数据处理\"></a>3.1 数据处理</h3><p>通过数据分析，我们发现对同个相同的问题id下面，相邻的两个问题id部分存在上下文关系。 所以为了让模型能够学习到更多的上下文关系我们在模型的输入加入了相邻的文本的信息：<br>[CLS]问题[SEP]问答[START]相邻的上个回答[INSERT]相邻的下个回答[SEP]</p>\n<h3 id=\"3-2-模型结构\"><a href=\"#3-2-模型结构\" class=\"headerlink\" title=\"3.2 模型结构\"></a>3.2 模型结构</h3><p>们设计了pytorch和tensorflow两个版本的模型。并且两个模型的单模型效果都在初赛上面进入了前5的成绩，由于两个模型不仅在框架上面，网络结构和输入上面也存在一定的差异，使得组队后，进行模型融合的时候，带来了比较大的提升。 两个模型融合直接进入了第二名的成绩和第一名差距也在一个千分点内。模型1:pytorch 版本，模型输入采用了下面的输入结构：[CLS]问题[SEP]问答[START]相邻的上个回答[INSERT]相邻的下个回答[SEP]。网络上面采用了bert等预训练模型作为特征提取层，由于bert不同层transformer提取出来的语义存在较大的差异，在不同语法上面侧重点不一样，我们设计了动态融合层，使用不同transformer层动态的加权方法来作为最后的表征。<br><img src=\"/image/beike3.png\" alt=\"pytorch model\"><br>图3：pytorch版本网络模型结构模型2: tensorflow版本。tf版模型将问题和回答按顺序拼接并使用“[SEP]”字符分割，并在问题前后插入“[unused1]”，回答前后按顺序插入“[unused2]”,“[unused3]”等字符后作为输入。模型可以大致分为编码层， 特征抽取层以及输出层。考虑到bert的不同层输出具有不同的语义信息，以及transformer对临近字符信息抽取能力较弱，编码层选择对bert各层输出加权融合，并输入bilstm强化对临近字符的抽取能力，最后选择bert各层的加权值以及bilstm的输出值的拼接向量作为编码层的输出。借鉴bert在信息抽取领域一些有效的结构，模型的特征抽取层被设计为：1.将[SEP],[unused1],[unused2]等特殊字符在编码层后的输出做为表征1；2.将回答，问题经过一层卷积和max-pooling的输出拼接值作为表征2；3. 将表征1，表征2拼接作为最终问题和回答的表征；4.最后将问题和回答的表征通过类似gate结构进行融合作为特征抽取层的输出。输出层简单的将上一层的输出输入dense加softmax得到是回答与不是回答的概率。<br><img src=\"/image/beike4.png\" alt=\"tensorflow model\"><br>图4：tensorflow版本网络模型结构</p>\n<h3 id=\"3-3提分Trick\"><a href=\"#3-3提分Trick\" class=\"headerlink\" title=\"3.3提分Trick\"></a>3.3提分Trick</h3><p>1.模型输入引入上下文信息提升巨大。<br>2.使用对抗学习带来5个千分点的提升。<br>3.在比赛数据语料上面继续做房产领域的的预训练带来5个千分点左右的提升。<br>4.使用动态加权层来融合预训练模型不同层的输出带来3个左右的千分点的提升。</p>\n<h3 id=\"3-4-模型融合\"><a href=\"#3-4-模型融合\" class=\"headerlink\" title=\"3.4 模型融合\"></a>3.4 模型融合</h3><p>我们pytorch和tensorflow两个版本的单模型都有着很好的效果在A榜能进入前5。后面由于竞争比较激烈，我们采用了多模型集成学习的方法，基于pytorch和tensorflow两个版本分别跑了bert-wwm和mac-bert两个预训练的交叉验证的结果作为特征，为了防止模型过拟合，采用了线形模型作为基模型。最终取得了A榜第二和B榜第二的成绩。和第一名仅仅差距在一个千分点内。</p>\n<h2 id=\"4-参考\"><a href=\"#4-参考\" class=\"headerlink\" title=\"4.参考\"></a>4.参考</h2><p>[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Association for Computational Linguistics (NAACL).</p>\n<p>[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000–6010.<br>[3] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. 2019a. Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101.<br>。<br>[4] Cui, Yiming  and Che, Wanxiang  and Liu, Ting  and Qin, Bing Wang, Shijin and Hu, Guoping . 2020. Revisiting Pre-Trained Models for chinese Natural Language Processing. EMNLP\t</p>\n","site":{"data":{}},"cover":"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7","excerpt":"<h2 id=\"摘要\"><a href=\"#摘要\" class=\"headerlink\" title=\"摘要\"></a>摘要</h2><p>房产行业聊天匹配问题本质上面是句子对的分类问题，即判断问题和回答是否匹配的文本分类问题。但是问答匹配不同的是除了依赖当前的回答，往往是在一个多伦对话场景下的产生背景，在本题目中通过对数据的分析，发现通过id问题下面的相邻id的问答具有上下文的关系，我们在设计模型的输入的时候引入了上下文的信息，使得模型能够学习到更多的因果关系。同时近年来基于transformer结构的预训练模型横扫各大nlp任务的SOTA，所以在模型结构方面我们也是采用了基于transformer的预训练语言模型结合任务做出了创新性的设计，取得了比较好的效果。我们的模型最终取得了线上第二的成绩。</p>\n<h2 id=\"关键词\"><a href=\"#关键词\" class=\"headerlink\" title=\"关键词\"></a>关键词</h2><p>文本分类、上下文信息、预训练模型、Transformer </p>","more":"<h2 id=\"1-任务简介\"><a href=\"#1-任务简介\" class=\"headerlink\" title=\"1.任务简介\"></a>1.任务简介</h2><p>给定IM交流片段，片段包含一个客户问题以及随后的经纪人若干IM消息，从这些随后的经纪人消息中找出一个是对客户问题的回答。简单来说即是判断问题和回答这个句子对是不是匹配的。</p>\n<h2 id=\"2-预训练语言模型简介\"><a href=\"#2-预训练语言模型简介\" class=\"headerlink\" title=\"2.预训练语言模型简介\"></a>2.预训练语言模型简介</h2><h3 id=\"2-1-BERT预训练模型\"><a href=\"#2-1-BERT预训练模型\" class=\"headerlink\" title=\"2.1 BERT预训练模型\"></a>2.1 BERT预训练模型</h3><p>2018年google公司AI团队新发布的BERT模型[1]，在机器阅读理解顶级水平测试SQuAD1.1中表现出惊人的成绩：全部两个衡量指标上全面超越人类，并且还在11种不同NLP测试中创出最佳成绩，包括将GLUE基准推至80.4％（绝对改进7.6％），MultiNLI准确度达到86.7% （绝对改进率5.6％）等。BERT为NLP带来了里程碑式的改变，也是NLP领域近年来最重要的进展。    Bert base版本使用了12层的transformer [2] encoder层部分作为编码器来对文本输入提取语义特征，用空间向量来表示。Transformer通过self-attention机制使得相同的字在不同的语境下面有不同的空间向量表示。同时相比于传统的rnn,cnn特征提取层来说，突破了rnn不能并行计算的限制，相比cnn计算两个位置之间的关联所需的操作次数不随长度的增长。<br>      <img src=\"/image/transformer.png\" alt=\"transformer\"><br>       图1：transformer网络结构Bert使用了两个预训练任务，来训练模型的权重。一个是mask language model， 即通过对输入的文本进行部分的随机替换成[MASK]字符，然后让模型来预测被[ MASK]的字符是什么，类似于完形填空任务。另外一个就是句子对分类任务，即判断相邻的句子是不是上下文关系。  </p>\n<pre><code>   ![bert_finetune](/image/bert_finetune.png)\n\n   图2：bert预训练和finetune\n   \n   \n   \n</code></pre>\n<h3 id=\"2-2-BERT-WWM预训练模型\"><a href=\"#2-2-BERT-WWM预训练模型\" class=\"headerlink\" title=\"2.2 BERT-WWM预训练模型\"></a>2.2 BERT-WWM预训练模型</h3><p>bert-wwm [3]是谷歌在2019年5月31日发布的一项BERT的升级版本，主要更改了原预训练阶段的训练样本生成策略。 简单来说，原有基于WordPiece的分词方式会把一个完整的词切分成若干个子词，在生成训练样本时，这些被分开的子词会随机被mask。 在全词Mask中，如果一个完整的词的部分WordPiece子词被mask，则同属该词的其他部分也会被mask，即全词Mask。</p>\n<h3 id=\"2-3-MACBERT预训练模型\"><a href=\"#2-3-MACBERT预训练模型\" class=\"headerlink\" title=\"2.3 MACBERT预训练模型\"></a>2.3 MACBERT预训练模型</h3><p>macbert [4]作者针对Bert在做MLM预训练的时候使用的 [MASK]替换输入，但是在做别的下游任务finetune的时候是没有[MASK]输入的，这就导致了预训练任务输入和finetune输入的差异问题。不使用[MASK]token进行mask，因为在token微调阶段从未出现过[MASK]，而是通过使用同义词替换的方法进行替换。</p>\n<h2 id=\"3-解决方案\"><a href=\"#3-解决方案\" class=\"headerlink\" title=\"3.解决方案\"></a>3.解决方案</h2><h3 id=\"3-1-数据处理\"><a href=\"#3-1-数据处理\" class=\"headerlink\" title=\"3.1 数据处理\"></a>3.1 数据处理</h3><p>通过数据分析，我们发现对同个相同的问题id下面，相邻的两个问题id部分存在上下文关系。 所以为了让模型能够学习到更多的上下文关系我们在模型的输入加入了相邻的文本的信息：<br>[CLS]问题[SEP]问答[START]相邻的上个回答[INSERT]相邻的下个回答[SEP]</p>\n<h3 id=\"3-2-模型结构\"><a href=\"#3-2-模型结构\" class=\"headerlink\" title=\"3.2 模型结构\"></a>3.2 模型结构</h3><p>们设计了pytorch和tensorflow两个版本的模型。并且两个模型的单模型效果都在初赛上面进入了前5的成绩，由于两个模型不仅在框架上面，网络结构和输入上面也存在一定的差异，使得组队后，进行模型融合的时候，带来了比较大的提升。 两个模型融合直接进入了第二名的成绩和第一名差距也在一个千分点内。模型1:pytorch 版本，模型输入采用了下面的输入结构：[CLS]问题[SEP]问答[START]相邻的上个回答[INSERT]相邻的下个回答[SEP]。网络上面采用了bert等预训练模型作为特征提取层，由于bert不同层transformer提取出来的语义存在较大的差异，在不同语法上面侧重点不一样，我们设计了动态融合层，使用不同transformer层动态的加权方法来作为最后的表征。<br><img src=\"/image/beike3.png\" alt=\"pytorch model\"><br>图3：pytorch版本网络模型结构模型2: tensorflow版本。tf版模型将问题和回答按顺序拼接并使用“[SEP]”字符分割，并在问题前后插入“[unused1]”，回答前后按顺序插入“[unused2]”,“[unused3]”等字符后作为输入。模型可以大致分为编码层， 特征抽取层以及输出层。考虑到bert的不同层输出具有不同的语义信息，以及transformer对临近字符信息抽取能力较弱，编码层选择对bert各层输出加权融合，并输入bilstm强化对临近字符的抽取能力，最后选择bert各层的加权值以及bilstm的输出值的拼接向量作为编码层的输出。借鉴bert在信息抽取领域一些有效的结构，模型的特征抽取层被设计为：1.将[SEP],[unused1],[unused2]等特殊字符在编码层后的输出做为表征1；2.将回答，问题经过一层卷积和max-pooling的输出拼接值作为表征2；3. 将表征1，表征2拼接作为最终问题和回答的表征；4.最后将问题和回答的表征通过类似gate结构进行融合作为特征抽取层的输出。输出层简单的将上一层的输出输入dense加softmax得到是回答与不是回答的概率。<br><img src=\"/image/beike4.png\" alt=\"tensorflow model\"><br>图4：tensorflow版本网络模型结构</p>\n<h3 id=\"3-3提分Trick\"><a href=\"#3-3提分Trick\" class=\"headerlink\" title=\"3.3提分Trick\"></a>3.3提分Trick</h3><p>1.模型输入引入上下文信息提升巨大。<br>2.使用对抗学习带来5个千分点的提升。<br>3.在比赛数据语料上面继续做房产领域的的预训练带来5个千分点左右的提升。<br>4.使用动态加权层来融合预训练模型不同层的输出带来3个左右的千分点的提升。</p>\n<h3 id=\"3-4-模型融合\"><a href=\"#3-4-模型融合\" class=\"headerlink\" title=\"3.4 模型融合\"></a>3.4 模型融合</h3><p>我们pytorch和tensorflow两个版本的单模型都有着很好的效果在A榜能进入前5。后面由于竞争比较激烈，我们采用了多模型集成学习的方法，基于pytorch和tensorflow两个版本分别跑了bert-wwm和mac-bert两个预训练的交叉验证的结果作为特征，为了防止模型过拟合，采用了线形模型作为基模型。最终取得了A榜第二和B榜第二的成绩。和第一名仅仅差距在一个千分点内。</p>\n<h2 id=\"4-参考\"><a href=\"#4-参考\" class=\"headerlink\" title=\"4.参考\"></a>4.参考</h2><p>[1] Jacob Devlin, Ming-Wei Chang, Kenton Lee, and Kristina Toutanova. 2019. BERT: Pre-training of deep bidirectional transformers for language understanding. In North American Association for Computational Linguistics (NAACL).</p>\n<p>[2] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia Polosukhin. 2017. Attention is all you need. In Advances in Neural Information Pro- cessing Systems, pages 6000–6010.<br>[3] Yiming Cui, Wanxiang Che, Ting Liu, Bing Qin, Ziqing Yang, Shijin Wang, and Guoping Hu. 2019a. Pre-training with whole word masking for chinese bert. arXiv preprint arXiv:1906.08101.<br>。<br>[4] Cui, Yiming  and Che, Wanxiang  and Liu, Ting  and Qin, Bing Wang, Shijin and Hu, Guoping . 2020. Revisiting Pre-Trained Models for chinese Natural Language Processing. EMNLP\t</p>"},{"layout":"blog","title":"Google QA Labeling 金牌方案分享","date":"2020-03-12T12:34:37.000Z","_content":"\n\n## 比赛背景与问题分析\n[google qa labeling 比赛链接](https://www.kaggle.com/c/google-quest-challenge)\n一个月前看到了kaggle上面google举办的一个nlp比赛，google quest Q&A labeling。就是根据问答对，预测30个不同对label值， 这些label值是人工给问答对打上对标签。标签值是0到1之间的数值评分。 这题既可以理解成回归的方法 也 可以理解成 分类的方法去做。  \n## 方案总结\n这次比赛中我们主要使用roberta large, roberta base, xlnet  也尝试过最新的google T5模型，发现效果不是很好。我们对文本清洗，模型结构对设计， loss修改，文本增强都进行了大量的参试。由于比赛public的测试集只有16%,所以我们并没有刻意的去拟合lb,也只是才用了较为简单的后处理方案。这使得我们在最终private开榜以后成功的升到了第5名，进入了奖金区。\n![最终的排行榜](/image/googleqa.png)\n<!--more-->\n\n## 具体细节\n由于个人比较懒这里就直接copy我在kaggle 论坛分享的这个比赛的我们队伍的解决方案。\n\nFirst of all I want to thank my teammates here.I will briefly introduce our solution here.\nmodels\n1.Model structure。we design different models structure. We mainly refer to the solution of ccf internet sentiment analysis，concat different cls embedding . here it is the link BDCI2019-SENTIMENT-CLASSIFICATION\n2.We found 30 labels through analysis, one is the question-related evaluation, and the other is the answer-related evaluation. In order to make the model learn better, we have designed the q model to remove the problem-related label and the a model to process the answer Related labels。 it is better than qa models.\n3.different model test. roberta base >roberta large >xlnet base >bert base > t5 base.\nPost-processing\nAnalysis and evaluation methods and competition data，we use 0,1 reseting way. it improve lb 0.05 or more.\nFeatures\n1.we want that our model learns features that are not only considered in text, so we add host and\ncategory embeeding features annd other Statistical Features。 it improve both cv and lb about 0.005.\nText clean\n1.We also did text cleaning to remove stop words and some symbols， it improve about 0.002\nStacking\n1.Our Best Private model scored 0.42787 ,but we dont't select it. it is stacking by roberta large and roberta base and xlnet base.\nblend.loc[:,targets] = roberta_large_oof_test.loc[:,targets].values*0.4+0.3*roberta_base_oof_test.loc[:,targets].values+\\ xlnet_base_oof_test.loc[:,targets].values*0.3\nstacking improve both cv and lb about 0.02 . it help much.\n","source":"_posts/Google-QA-Labeling-金牌方案分享.md","raw":"---\nlayout: blog\ntitle: Google QA Labeling 金牌方案分享\ndate: 2020-03-12 20:34:37\ntags:\n  - nlp\n  - kaggle\ncategories:\n  - competitions\n  \n---\n\n\n## 比赛背景与问题分析\n[google qa labeling 比赛链接](https://www.kaggle.com/c/google-quest-challenge)\n一个月前看到了kaggle上面google举办的一个nlp比赛，google quest Q&A labeling。就是根据问答对，预测30个不同对label值， 这些label值是人工给问答对打上对标签。标签值是0到1之间的数值评分。 这题既可以理解成回归的方法 也 可以理解成 分类的方法去做。  \n## 方案总结\n这次比赛中我们主要使用roberta large, roberta base, xlnet  也尝试过最新的google T5模型，发现效果不是很好。我们对文本清洗，模型结构对设计， loss修改，文本增强都进行了大量的参试。由于比赛public的测试集只有16%,所以我们并没有刻意的去拟合lb,也只是才用了较为简单的后处理方案。这使得我们在最终private开榜以后成功的升到了第5名，进入了奖金区。\n![最终的排行榜](/image/googleqa.png)\n<!--more-->\n\n## 具体细节\n由于个人比较懒这里就直接copy我在kaggle 论坛分享的这个比赛的我们队伍的解决方案。\n\nFirst of all I want to thank my teammates here.I will briefly introduce our solution here.\nmodels\n1.Model structure。we design different models structure. We mainly refer to the solution of ccf internet sentiment analysis，concat different cls embedding . here it is the link BDCI2019-SENTIMENT-CLASSIFICATION\n2.We found 30 labels through analysis, one is the question-related evaluation, and the other is the answer-related evaluation. In order to make the model learn better, we have designed the q model to remove the problem-related label and the a model to process the answer Related labels。 it is better than qa models.\n3.different model test. roberta base >roberta large >xlnet base >bert base > t5 base.\nPost-processing\nAnalysis and evaluation methods and competition data，we use 0,1 reseting way. it improve lb 0.05 or more.\nFeatures\n1.we want that our model learns features that are not only considered in text, so we add host and\ncategory embeeding features annd other Statistical Features。 it improve both cv and lb about 0.005.\nText clean\n1.We also did text cleaning to remove stop words and some symbols， it improve about 0.002\nStacking\n1.Our Best Private model scored 0.42787 ,but we dont't select it. it is stacking by roberta large and roberta base and xlnet base.\nblend.loc[:,targets] = roberta_large_oof_test.loc[:,targets].values*0.4+0.3*roberta_base_oof_test.loc[:,targets].values+\\ xlnet_base_oof_test.loc[:,targets].values*0.3\nstacking improve both cv and lb about 0.02 . it help much.\n","slug":"Google-QA-Labeling-金牌方案分享","published":1,"updated":"2020-03-12T12:34:37.000Z","comments":1,"photos":[],"link":"","_id":"cl6kd8j8m00030vs6f5ls53mv","content":"<h2 id=\"比赛背景与问题分析\"><a href=\"#比赛背景与问题分析\" class=\"headerlink\" title=\"比赛背景与问题分析\"></a>比赛背景与问题分析</h2><p><a href=\"https://www.kaggle.com/c/google-quest-challenge\">google qa labeling 比赛链接</a><br>一个月前看到了kaggle上面google举办的一个nlp比赛，google quest Q&amp;A labeling。就是根据问答对，预测30个不同对label值， 这些label值是人工给问答对打上对标签。标签值是0到1之间的数值评分。 这题既可以理解成回归的方法 也 可以理解成 分类的方法去做。  </p>\n<h2 id=\"方案总结\"><a href=\"#方案总结\" class=\"headerlink\" title=\"方案总结\"></a>方案总结</h2><p>这次比赛中我们主要使用roberta large, roberta base, xlnet  也尝试过最新的google T5模型，发现效果不是很好。我们对文本清洗，模型结构对设计， loss修改，文本增强都进行了大量的参试。由于比赛public的测试集只有16%,所以我们并没有刻意的去拟合lb,也只是才用了较为简单的后处理方案。这使得我们在最终private开榜以后成功的升到了第5名，进入了奖金区。<br><img src=\"/image/googleqa.png\" alt=\"最终的排行榜\"></p>\n<span id=\"more\"></span>\n\n<h2 id=\"具体细节\"><a href=\"#具体细节\" class=\"headerlink\" title=\"具体细节\"></a>具体细节</h2><p>由于个人比较懒这里就直接copy我在kaggle 论坛分享的这个比赛的我们队伍的解决方案。</p>\n<p>First of all I want to thank my teammates here.I will briefly introduce our solution here.<br>models<br>1.Model structure。we design different models structure. We mainly refer to the solution of ccf internet sentiment analysis，concat different cls embedding . here it is the link BDCI2019-SENTIMENT-CLASSIFICATION<br>2.We found 30 labels through analysis, one is the question-related evaluation, and the other is the answer-related evaluation. In order to make the model learn better, we have designed the q model to remove the problem-related label and the a model to process the answer Related labels。 it is better than qa models.<br>3.different model test. roberta base &gt;roberta large &gt;xlnet base &gt;bert base &gt; t5 base.<br>Post-processing<br>Analysis and evaluation methods and competition data，we use 0,1 reseting way. it improve lb 0.05 or more.<br>Features<br>1.we want that our model learns features that are not only considered in text, so we add host and<br>category embeeding features annd other Statistical Features。 it improve both cv and lb about 0.005.<br>Text clean<br>1.We also did text cleaning to remove stop words and some symbols， it improve about 0.002<br>Stacking<br>1.Our Best Private model scored 0.42787 ,but we dont’t select it. it is stacking by roberta large and roberta base and xlnet base.<br>blend.loc[:,targets] &#x3D; roberta_large_oof_test.loc[:,targets].values<em>0.4+0.3</em>roberta_base_oof_test.loc[:,targets].values+\\ xlnet_base_oof_test.loc[:,targets].values*0.3<br>stacking improve both cv and lb about 0.02 . it help much.</p>\n","site":{"data":{}},"cover":"data:image/gif;base64,R0lGODlhAQABAIAAAAAAAP///yH5BAEAAAAALAAAAAABAAEAAAIBRAA7","excerpt":"<h2 id=\"比赛背景与问题分析\"><a href=\"#比赛背景与问题分析\" class=\"headerlink\" title=\"比赛背景与问题分析\"></a>比赛背景与问题分析</h2><p><a href=\"https://www.kaggle.com/c/google-quest-challenge\">google qa labeling 比赛链接</a><br>一个月前看到了kaggle上面google举办的一个nlp比赛，google quest Q&amp;A labeling。就是根据问答对，预测30个不同对label值， 这些label值是人工给问答对打上对标签。标签值是0到1之间的数值评分。 这题既可以理解成回归的方法 也 可以理解成 分类的方法去做。  </p>\n<h2 id=\"方案总结\"><a href=\"#方案总结\" class=\"headerlink\" title=\"方案总结\"></a>方案总结</h2><p>这次比赛中我们主要使用roberta large, roberta base, xlnet  也尝试过最新的google T5模型，发现效果不是很好。我们对文本清洗，模型结构对设计， loss修改，文本增强都进行了大量的参试。由于比赛public的测试集只有16%,所以我们并没有刻意的去拟合lb,也只是才用了较为简单的后处理方案。这使得我们在最终private开榜以后成功的升到了第5名，进入了奖金区。<br><img src=\"/image/googleqa.png\" alt=\"最终的排行榜\"></p>","more":"<h2 id=\"具体细节\"><a href=\"#具体细节\" class=\"headerlink\" title=\"具体细节\"></a>具体细节</h2><p>由于个人比较懒这里就直接copy我在kaggle 论坛分享的这个比赛的我们队伍的解决方案。</p>\n<p>First of all I want to thank my teammates here.I will briefly introduce our solution here.<br>models<br>1.Model structure。we design different models structure. We mainly refer to the solution of ccf internet sentiment analysis，concat different cls embedding . here it is the link BDCI2019-SENTIMENT-CLASSIFICATION<br>2.We found 30 labels through analysis, one is the question-related evaluation, and the other is the answer-related evaluation. In order to make the model learn better, we have designed the q model to remove the problem-related label and the a model to process the answer Related labels。 it is better than qa models.<br>3.different model test. roberta base &gt;roberta large &gt;xlnet base &gt;bert base &gt; t5 base.<br>Post-processing<br>Analysis and evaluation methods and competition data，we use 0,1 reseting way. it improve lb 0.05 or more.<br>Features<br>1.we want that our model learns features that are not only considered in text, so we add host and<br>category embeeding features annd other Statistical Features。 it improve both cv and lb about 0.005.<br>Text clean<br>1.We also did text cleaning to remove stop words and some symbols， it improve about 0.002<br>Stacking<br>1.Our Best Private model scored 0.42787 ,but we dont’t select it. it is stacking by roberta large and roberta base and xlnet base.<br>blend.loc[:,targets] &#x3D; roberta_large_oof_test.loc[:,targets].values<em>0.4+0.3</em>roberta_base_oof_test.loc[:,targets].values+\\ xlnet_base_oof_test.loc[:,targets].values*0.3<br>stacking improve both cv and lb about 0.02 . it help much.</p>"}],"PostAsset":[],"PostCategory":[{"post_id":"cl6kd8j8j00010vs69odh859q","category_id":"cl6kd8j8n00040vs6873obs6r","_id":"cl6kd8j8t000c0vs67bafdcqm"},{"post_id":"cl6kd8j8m00030vs6f5ls53mv","category_id":"cl6kd8j8n00040vs6873obs6r","_id":"cl6kd8j8t000e0vs6ejfz0da6"}],"PostTag":[{"post_id":"cl6kd8j8j00010vs69odh859q","tag_id":"cl6kd8j8o00050vs6b7mhepw9","_id":"cl6kd8j8u000g0vs68h5wdsu8"},{"post_id":"cl6kd8j8j00010vs69odh859q","tag_id":"cl6kd8j8r000a0vs6hzl653sz","_id":"cl6kd8j8u000h0vs61kgh6n1q"},{"post_id":"cl6kd8j8j00010vs69odh859q","tag_id":"cl6kd8j8t000d0vs698ribnql","_id":"cl6kd8j8u000j0vs6bnq39839"},{"post_id":"cl6kd8j8m00030vs6f5ls53mv","tag_id":"cl6kd8j8o00050vs6b7mhepw9","_id":"cl6kd8j8v000k0vs6hx4khr5c"},{"post_id":"cl6kd8j8m00030vs6f5ls53mv","tag_id":"cl6kd8j8u000i0vs6h0qq0nq1","_id":"cl6kd8j8v000l0vs681fcbpkx"}],"Tag":[{"name":"nlp","_id":"cl6kd8j8o00050vs6b7mhepw9"},{"name":"text classifier","_id":"cl6kd8j8r000a0vs6hzl653sz"},{"name":"qa match","_id":"cl6kd8j8t000d0vs698ribnql"},{"name":"kaggle","_id":"cl6kd8j8u000i0vs6h0qq0nq1"}]}}